{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "To carry out testing of the proposed pipeline to detect the tongue using OpenCV\n",
    "\n",
    "Pipeline: Image > CLAHE > Image thresholding > detect contours of image > use contours to detect centre of tongue\n",
    "\n",
    "Thereafter, we apply the SAM2 model on the image and see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install if running on colab\n",
    "# !pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics import SAM\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tongue_center(image_path):\n",
    "    '''\n",
    "    Finds the center of the tongue. Process:\n",
    "    1. read image, convert into grayscale\n",
    "    2. Apply CLAHE to enhance image features\n",
    "    3. Apply binary thresholding\n",
    "    4. Look for contours in the thresholded image\n",
    "    5. Get the largest contour, then find its centroid by computing moments\n",
    "\n",
    "    Args:\n",
    "        image_path (str): path to the image\n",
    "\n",
    "    Returns:\n",
    "        list of ints: (x, y) coordinates of center of tongue. If unable to find center of tongue, entries will be\n",
    "        [-1, -1] instead\n",
    "    '''\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Enhance contrast\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "\n",
    "    # Threshold the image\n",
    "    _, binary = cv2.threshold(enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        # Find the largest contour\n",
    "        tongue_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Compute the center of the tongue\n",
    "        M = cv2.moments(tongue_contour)\n",
    "        if M[\"m00\"] != 0:\n",
    "            cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "        else:\n",
    "            cx, cy = 0, 0\n",
    "\n",
    "        # Visualize the result\n",
    "        # result = image.copy()\n",
    "        # cv2.drawContours(result, [tongue_contour], -1, (0, 255, 0), 2)\n",
    "        # cv2.circle(result, (cx, cy), 5, (0, 0, 255), -1)\n",
    "        # cv2.imwrite(f\"./processed/{image_path[10:]}\", result)\n",
    "        # cv2.imshow(\"Detected Tongue Center\", result)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "\n",
    "        return [cx, cy]\n",
    "    else:\n",
    "        print(f\"No contours found for image path: {image_path}\")\n",
    "        # return [-1, -1] to invalidate return value, and throw error if we access the values\n",
    "        return [-1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = []\n",
    "\n",
    "for i in range(1, 10):\n",
    "    cx, cy = find_tongue_center(f\"./Samples/sample_{i}.jpg\")\n",
    "    coords.append((cx, cy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(318, 363),\n",
       " (320, 300),\n",
       " (380, 239),\n",
       " (301, 308),\n",
       " (334, 266),\n",
       " (254, 303),\n",
       " (324, 349),\n",
       " (312, 278),\n",
       " (300, 423)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image_path = \"./Samples/sample_7.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Enhance contrast\n",
    "clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "enhanced = clahe.apply(gray)\n",
    "\n",
    "# Threshold the image\n",
    "_, binary = cv2.threshold(enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# Find contours\n",
    "contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "contour_lst = []\n",
    "\n",
    "for contour in contours:\n",
    "  M = cv2.moments(contour)\n",
    "  if M[\"m00\"] != 0:\n",
    "      cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "      cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "      contour_lst.append([cx, cy])\n",
    "\n",
    "for lst in contour_lst:\n",
    "  cx = lst[0]\n",
    "  cy = lst[1]\n",
    "  cv2.circle(image, (cx, cy), 5, (0, 0, 255), -1)\n",
    "\n",
    "cv2.imshow(\"image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SAM2 to segment the tongues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAM(\"sam2.1_l.pt\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# print out device we are working on\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(image, center):\n",
    "    '''\n",
    "    Uses the SAM2 model to segment the tongue out\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image): image as read from PIL image library\n",
    "        center (list): list of coordinates for center of the tongue\n",
    "\n",
    "    Returns:\n",
    "        segmented_image (np.array): image after performing segmentation and masking\n",
    "    '''\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    # Segmentation: arbitrarily take 0, 0 as a point that does not contain the tongue\n",
    "    results_pil = model(image_np, points=[center, [0, 0]], labels=[1, 0])\n",
    "\n",
    "    # Get the mask from the results\n",
    "    mask_pil = results_pil[0].masks.data[0].cpu().numpy()\n",
    "\n",
    "    # Masking\n",
    "    binary_mask = mask_pil > 0.5\n",
    "    rgb_mask = np.repeat(binary_mask[:, :, np.newaxis], 3, axis=2)\n",
    "    segmented_image = image_np * rgb_mask\n",
    "\n",
    "    return segmented_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read images into a list of images\n",
    "im_lst = []\n",
    "\n",
    "for i in range(1, 10):\n",
    "    im = Image.open(f\"./Samples/sample_{i}.jpg\")\n",
    "    im_lst.append(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
